# GWAS script

This file contains the commands used for the data processing after the SNPs' imputation through the Michigan Imputation Server. The resulting .zip files from each chromosome were transformed and merged into one file that was analysed to obtain an overview of the data: we computed basic statistics of the data and performed a population stratification analysis again.
After the overview of the data, files were transformed to GAPIT compatible format: HapMap, and the dataset was divided in training and validation (La Princesa, La Paz and Clínico; and Valcecillas respectively). Once the data was prepared, the GWAS was performed using [GAPIT](https://www.maizegenetics.net/gapit). From this analysis we found which SNPs associated to BMI and rheumatic arthritis.

 ## Get all required data and tools  
 Download the required R packiges pointed in the [GAPIT](https://www.maizegenetics.net/gapit) manual tool
 
    # Decompress files (for mac)
    brew install p7zip
    
    # DosageConvertor (refer to https://github.com/Santy-8128/DosageConvertor)
    brew install cmake
    sudo easy-install pip
    pip install --user cget --ignore-installed six
    cd DosageConvertor
    bash install.sh
       
    
    
 ## Data preparation
 ### 1. Merging files
The Michigan Imputation Server produces a zip file for each chromosome; in order to work with them all in GAPIT, we need the in HapMap format, plus to analyse then with plinkas we did before we will need to transform them too into a plink readable format. The files must be uncompressed using 7-Zip program:
   
     # In ctr/case directory:
     for file in chr*.zip; do 7z e "${file}" -p'PASSWORD'; done

The output consists in two files: a '.dose.vcf.gz' and a '.info.gz' for each chromosome. In order to merge all chromosome files into one single file, we can use [bcftools](http://samtools.github.io/bcftools/bcftools.html):

     # Obtain PLINK files
     for TestDataImputedVCF in Directory; do Prefix="${TestDataImputedVCF%.*.*.*}";  \
     ./DosageConvertor --vcfDose "${TestDataImputedVCF}.dose.vcf.gz" \
     -info "${TestDataImputedVCF}.info.gz" \
     --prefix "Path/${Prefix##*/}" \
     --type plink \
     --format 1; done
                               
                          
     # Transform files to VCF
     ./plink2 --import-dosage chr*.plink.dosage.gz \
     --map chr*.plink.map \
     --fam chr*.plink.fam  \
     --recode vcf --out OutPrefix
     
     # Join VCF files and compress
     bcftools concat ctrl/chr{1..22}.vcf -O z -o ctrl.vcf       
     bgzip -c ctrl.vcf > ctrl.vcf.gz


### 2. Quality analysis
In this step we filter the imputed data, keeping only the SNPs we found to be in equlibrium before. We also apply the same filters as before (maf, geno, mind).


**2.1 Pre-filtering**  
We extract only the SNPs in equlibrium found in the case files prior imputation. The IDs of the SNPs have changed and are no longer rs* but chr:position:nt:nt, so we need first to obtain the IDs of the SNPs and then extract those SNPs from the cases vcf imputed. We also filter out SNPs with minor allele frequency under 5%. 

    # Obtain SNP's IDs
    awk ' { print $1":"$4":"$5”:”$6} ' case_postFilter_bim >> <matchSNP.txt> #all SNPs passed the filter so we can obtain the IDs from this file
    
    # Filtering
    ./plink --vcf /Volumes/SSD/vcfs/case/vcf_files/case.vcf.gz --extract <matchSNP.txt> --allow-no-sex --make-bed --double-id --out <filtered_cases>
    
The final filtering step is extracting the genotypes for which we have phenotypic data. Using R code, I take the ID column from the excel file with the phenotype's data, and I create a tsv with famili ID in the first column (1 for all), and the patient code in the second column. Then I extract those profiles from my genotypes' file using PLINK:

    #Extract genotypes
    ./plink --bfile <filtered_cases> --keep <fam_ID.txt> --allow-no-sex --make-bed --recode vcf --double-id --out <filtExtractCases>
    
  
    
**2.2 Get basic statistics**

    ./plink --bfile <case_plink> --freqx --maf 0.05 --missing --het --make-bed --allow-no-sex --double-id --out <case_analysis>
    
    # Singleton count
    awk '($6==1)' case_analysis.frqx | awk -F "\t" '{ if(($5 == 0) || ($7 == 0)) {print} }' | wc -l
    
    # Missing data
    awk '{ total += $5; count++ } END { print total/count }' case_analysis.lmiss # case SNPs 
    awk '{ total += $6; count++ } END { print total/count }' case_analysis.imiss # case genotypes 
   
    
    # Heterozygosis 
    awk '{ total += $6 } END { print total/NR }' case_analysis.het
    
Population stratification is also checked using the same R code as before.  



### 3. Data preparation
We transform into GAPIT compatible format (HapMap) and divide the dataset into training and validation. Now we only work with the case files.
 
 **3.1 Transform to HapMap**  
Genotype information in GAPIT must be imported in either HapMap. We can transform the vcf to HapMap using TASSEL (download [here](https://www.maizegenetics.net/tassel), info about the source [here](https://bitbucket.org/tasseladmin/tassel-5-source/src/master)), a tool by the creators of GAPIT. To transform the file, go to the directory to where TASSEL was download and run the following command:
     
    ./run_pipeline.pl -Xmx5g -fork1 -vcf <filtExtractCases.vcf> -export -exportType Hapmap


Performing the GWAS using the tool QTCAT requires a Diploid HapMap, which can be created with the following command line:

    ./run_pipeline.pl -Xmx5g -fork1 -vcf <filtExtractCases.vcf> -export <outputFile> -exportType HapmapDiploid



 **3.2 Divide Dataset**  
 The dataset is divided in order to obtain a subdataset for training and another validation. The samples collected for Valdecillas hospital are substracted from the whole and two new files are created using R code. The characteristics of the phenotypic data, as well as the population structure, are analysed and compared to stablish the suitability of the split and reject the random split alternative. Prior this step, a phenotypics traits association had been carried out (go to phenotypeAnalysis.md). 

    # Commands to extract patiens IDs from genotype data
    cat trainCases.fam | awk '{print $2}' > trainIDs.txt
    cat valCases.fam | awk '{print $2}' > valIDs.txt
    
    # Command for PCA analysis
    ./plink --bfile <trainCases> --pca --out <trainPCA>
    ./plink --bfile <valCases> --pca --out <valPCA>


## GAPIT 
The files must be transformed into HapMap format (using ```TASSEL```), and after all data has been analysed and transformed into the right formt, the GAPIT analysis is performed using an R code that can be found in this repository as ```3.2-GAPIT.R``` 
